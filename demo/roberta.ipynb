{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.data.roberta_dataset import RobertaTokenizeDataset\n",
    "from mda.data.data_collection import DataCollection\n",
    "from mda.api import predict\n",
    "from mda.model.roberta import RobertaClassifier\n",
    "from mda.util import load_json, AUTO_DEVICE\n",
    "from repo_root import get_full_path\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DOMAINS = [\n",
    "    \"deathpenalty\",\n",
    "    \"guncontrol\",\n",
    "    \"immigration\",\n",
    "    \"samesex\",\n",
    "    \"tobacco\",\n",
    "]\n",
    "OUT_DOMAINS = [\"climate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the lexicon on labeled, in domain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collection = DataCollection.parse_obj(\n",
    "    load_json(get_full_path(\"data/mfc.test.json\"))\n",
    ")\n",
    "in_domain_test_dataset = RobertaTokenizeDataset(\n",
    "    batch_size=100,\n",
    "    num_workers=8,\n",
    "    collection=test_collection,\n",
    "    use_domain_strs=IN_DOMAINS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = RobertaClassifier(\n",
    "    n_classes=len(test_collection.class_strs),\n",
    "    n_domains=len(test_collection.domain_strs),\n",
    "    use_domain_specific_bias=True,\n",
    ")\n",
    "model.load_state_dict(\n",
    "    torch.load(\n",
    "        get_full_path(\"wkdir/holdout_domain/mfc/roberta_dsbias/climate/checkpoint.pth\")\n",
    "    )\n",
    ")\n",
    "model = model.to(AUTO_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:10<00:00,  1.90it/s]\n"
     ]
    }
   ],
   "source": [
    "probs = predict(model, in_domain_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7049999833106995"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.argmax(probs, dim=-1)\n",
    "acc = (\n",
    "    (\n",
    "        preds.cpu()\n",
    "        == torch.cat(\n",
    "            [\n",
    "                batch[\"class_idx\"]\n",
    "                for batch in in_domain_test_dataset.get_loader(shuffle=False)\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "    )\n",
    "    * 1.0\n",
    ").mean()\n",
    "acc.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the lexicon on labeled, out of domain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 38/38 [00:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6511198878288269"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_collection = DataCollection.parse_obj(\n",
    "    load_json(get_full_path(\"data/mfc.train.json\"))\n",
    ")\n",
    "out_domain_test_dataset = RobertaTokenizeDataset(\n",
    "    batch_size=100,\n",
    "    num_workers=10,\n",
    "    collection=train_collection,\n",
    "    use_domain_strs=OUT_DOMAINS,\n",
    ")\n",
    "probs = predict(model, out_domain_test_dataset)\n",
    "preds = torch.argmax(probs, dim=-1)\n",
    "acc = (\n",
    "    (\n",
    "        preds.cpu()\n",
    "        == torch.cat(\n",
    "            [batch[\"class_idx\"] for batch in out_domain_test_dataset.get_loader(False)],\n",
    "            dim=0,\n",
    "        )\n",
    "    )\n",
    "    * 1.0\n",
    ").mean()\n",
    "acc.item()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "841b8c7c706223fb03b727be38bf91caf98d995881f3f0bfeccf303ad7e8f138"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
