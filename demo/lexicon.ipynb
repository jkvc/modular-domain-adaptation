{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.data.bow_dataset import BagOfWordsSingleBatchDataset\n",
    "from mda.data.data_collection import DataCollection\n",
    "from mda.logreg import lexicon_predict, train_lexicon\n",
    "from mda.util import load_json\n",
    "from repo_root import get_full_path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DOMAINS = [\n",
    "    \"deathpenalty\",\n",
    "    \"guncontrol\",\n",
    "    \"immigration\",\n",
    "    \"samesex\",\n",
    "    \"tobacco\",\n",
    "]\n",
    "OUT_DOMAINS = [\"climate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collection = DataCollection.parse_obj(\n",
    "    load_json(get_full_path(\"data/mfc.train.json\"))\n",
    ")\n",
    "train_dataset = BagOfWordsSingleBatchDataset(\n",
    "    batch_size=-1,\n",
    "    num_workers=-1,\n",
    "    collection=train_collection,\n",
    "    use_domain_strs=IN_DOMAINS,\n",
    "    vocab_size=5000,\n",
    ")\n",
    "vocab = train_dataset.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab size\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['said',\n",
       " 'gun',\n",
       " 'would',\n",
       " 'state',\n",
       " 'new',\n",
       " 'court',\n",
       " 'law',\n",
       " 'marriage',\n",
       " 'states',\n",
       " 'people']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common vocab words\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:13<00:00, 375.66it/s]\n"
     ]
    }
   ],
   "source": [
    "lexicon_df = train_lexicon(dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Economic</th>\n",
       "      <th>Capacity and Resources</th>\n",
       "      <th>Morality</th>\n",
       "      <th>Fairness and Equality</th>\n",
       "      <th>Legality, Constitutionality, Jurisdiction</th>\n",
       "      <th>Policy Prescription and Evaluation</th>\n",
       "      <th>Crime and Punishment</th>\n",
       "      <th>Security and Defense</th>\n",
       "      <th>Health and Safety</th>\n",
       "      <th>Quality of Life</th>\n",
       "      <th>Cultural Identity</th>\n",
       "      <th>Public Sentiment</th>\n",
       "      <th>Political</th>\n",
       "      <th>External Regulation and Reputation</th>\n",
       "      <th>Other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said</td>\n",
       "      <td>0.067012</td>\n",
       "      <td>0.179501</td>\n",
       "      <td>-0.144857</td>\n",
       "      <td>-5.015747e-03</td>\n",
       "      <td>-0.003872</td>\n",
       "      <td>-0.344471</td>\n",
       "      <td>0.090793</td>\n",
       "      <td>-0.010305</td>\n",
       "      <td>-0.034390</td>\n",
       "      <td>0.076832</td>\n",
       "      <td>-0.156841</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>-0.005524</td>\n",
       "      <td>0.239455</td>\n",
       "      <td>2.972137e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gun</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.112756</td>\n",
       "      <td>1.091244e-04</td>\n",
       "      <td>-0.123926</td>\n",
       "      <td>-0.209876</td>\n",
       "      <td>-0.073423</td>\n",
       "      <td>-0.000035</td>\n",
       "      <td>0.187286</td>\n",
       "      <td>-0.082601</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.176232</td>\n",
       "      <td>-0.026121</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>-7.774811e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>would</td>\n",
       "      <td>-0.000088</td>\n",
       "      <td>0.060649</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-1.287555e-02</td>\n",
       "      <td>-0.002946</td>\n",
       "      <td>0.514685</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>-0.091849</td>\n",
       "      <td>0.008815</td>\n",
       "      <td>-0.008114</td>\n",
       "      <td>-0.232657</td>\n",
       "      <td>-0.139944</td>\n",
       "      <td>0.134798</td>\n",
       "      <td>0.077551</td>\n",
       "      <td>-5.086828e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>state</td>\n",
       "      <td>-0.199858</td>\n",
       "      <td>0.142883</td>\n",
       "      <td>-0.127448</td>\n",
       "      <td>-1.527587e-02</td>\n",
       "      <td>0.080521</td>\n",
       "      <td>0.045492</td>\n",
       "      <td>-0.074152</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>-0.000861</td>\n",
       "      <td>-0.021786</td>\n",
       "      <td>0.028198</td>\n",
       "      <td>-0.124993</td>\n",
       "      <td>0.240542</td>\n",
       "      <td>-1.417165e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new</td>\n",
       "      <td>0.144268</td>\n",
       "      <td>-0.076395</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>-1.417813e-05</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.153550</td>\n",
       "      <td>-0.174919</td>\n",
       "      <td>0.085598</td>\n",
       "      <td>-0.055154</td>\n",
       "      <td>-0.000029</td>\n",
       "      <td>0.055116</td>\n",
       "      <td>0.069544</td>\n",
       "      <td>-0.220090</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>-3.173600e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>sworn</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>8.142626e-02</td>\n",
       "      <td>0.176276</td>\n",
       "      <td>-0.000097</td>\n",
       "      <td>-0.170792</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>2.918449e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>commercials</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>6.154087e-05</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>-0.049537</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>-3.333278e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>profiling</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>4.399460e-03</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.020404</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.000071</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.000065</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>1.055479e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>auto</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>3.308873e-07</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>-0.000057</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-6.898679e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>tables</td>\n",
       "      <td>0.067666</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>4.456401e-05</td>\n",
       "      <td>-0.000053</td>\n",
       "      <td>-0.007310</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>-0.000060</td>\n",
       "      <td>-0.000023</td>\n",
       "      <td>-0.028401</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>1.853706e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  Economic  Capacity and Resources  Morality  \\\n",
       "0            said  0.067012                0.179501 -0.144857   \n",
       "1             gun  0.000019                0.000013  0.112756   \n",
       "2           would -0.000088                0.060649 -0.000031   \n",
       "3           state -0.199858                0.142883 -0.127448   \n",
       "4             new  0.144268               -0.076395  0.000052   \n",
       "...           ...       ...                     ...       ...   \n",
       "4995        sworn  0.000054               -0.000019  0.000008   \n",
       "4996  commercials -0.000060                0.000019  0.000015   \n",
       "4997    profiling -0.000007                0.000016 -0.000033   \n",
       "4998         auto -0.000066               -0.000009 -0.000013   \n",
       "4999       tables  0.067666                0.000016  0.000012   \n",
       "\n",
       "      Fairness and Equality  Legality, Constitutionality, Jurisdiction  \\\n",
       "0             -5.015747e-03                                  -0.003872   \n",
       "1              1.091244e-04                                  -0.123926   \n",
       "2             -1.287555e-02                                  -0.002946   \n",
       "3             -1.527587e-02                                   0.080521   \n",
       "4             -1.417813e-05                                   0.000038   \n",
       "...                     ...                                        ...   \n",
       "4995           8.142626e-02                                   0.176276   \n",
       "4996           6.154087e-05                                  -0.000066   \n",
       "4997           4.399460e-03                                   0.000137   \n",
       "4998           3.308873e-07                                   0.000060   \n",
       "4999           4.456401e-05                                  -0.000053   \n",
       "\n",
       "      Policy Prescription and Evaluation  Crime and Punishment  \\\n",
       "0                              -0.344471              0.090793   \n",
       "1                              -0.209876             -0.073423   \n",
       "2                               0.514685              0.000102   \n",
       "3                               0.045492             -0.074152   \n",
       "4                               0.153550             -0.174919   \n",
       "...                                  ...                   ...   \n",
       "4995                           -0.000097             -0.170792   \n",
       "4996                            0.000003             -0.000079   \n",
       "4997                           -0.000019             -0.020404   \n",
       "4998                           -0.000113             -0.000004   \n",
       "4999                           -0.007310              0.000079   \n",
       "\n",
       "      Security and Defense  Health and Safety  Quality of Life  \\\n",
       "0                -0.010305          -0.034390         0.076832   \n",
       "1                -0.000035           0.187286        -0.082601   \n",
       "2                -0.091849           0.008815        -0.008114   \n",
       "3                 0.000056           0.000041        -0.000861   \n",
       "4                 0.085598          -0.055154        -0.000029   \n",
       "...                    ...                ...              ...   \n",
       "4995              0.000002           0.000008         0.000008   \n",
       "4996              0.000013           0.000010         0.000084   \n",
       "4997              0.000060          -0.000071        -0.000019   \n",
       "4998             -0.000022          -0.000031         0.000097   \n",
       "4999              0.000039           0.000018         0.000036   \n",
       "\n",
       "      Cultural Identity  Public Sentiment  Political  \\\n",
       "0             -0.156841         -0.000014  -0.005524   \n",
       "1             -0.000007          0.176232  -0.026121   \n",
       "2             -0.232657         -0.139944   0.134798   \n",
       "3             -0.021786          0.028198  -0.124993   \n",
       "4              0.055116          0.069544  -0.220090   \n",
       "...                 ...               ...        ...   \n",
       "4995          -0.000010         -0.000019  -0.000065   \n",
       "4996          -0.049537          0.000050   0.000008   \n",
       "4997          -0.000065          0.000061   0.000027   \n",
       "4998           0.000109          0.000016  -0.000057   \n",
       "4999          -0.000060         -0.000023  -0.028401   \n",
       "\n",
       "      External Regulation and Reputation         Other  \n",
       "0                               0.239455  2.972137e-05  \n",
       "1                               0.000043 -7.774811e-06  \n",
       "2                               0.077551 -5.086828e-05  \n",
       "3                               0.240542 -1.417165e-05  \n",
       "4                               0.000067 -3.173600e-05  \n",
       "...                                  ...           ...  \n",
       "4995                           -0.000008  2.918449e-05  \n",
       "4996                            0.000033 -3.333278e-06  \n",
       "4997                           -0.000033  1.055479e-06  \n",
       "4998                           -0.000002 -6.898679e-08  \n",
       "4999                            0.000055  1.853706e-09  \n",
       "\n",
       "[5000 rows x 16 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('economic', 0.9426248669624329),\n",
       " ('business', 0.9138430953025818),\n",
       " ('economy', 0.8519132137298584),\n",
       " ('financial', 0.8444198966026306),\n",
       " ('costs', 0.7670855522155762),\n",
       " ('jobs', 0.764188289642334),\n",
       " ('budget', 0.7124324440956116),\n",
       " ('income', 0.7074215412139893),\n",
       " ('tax', 0.6934945583343506),\n",
       " ('sales', 0.6821240782737732)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words with highest weights in frame \"Economics\"\n",
    "sorted(\n",
    "    list(zip(lexicon_df[\"word\"].to_list(), lexicon_df[\"Economic\"].to_list())),\n",
    "    reverse=True,\n",
    "    key=lambda x: x[1],\n",
    ")[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the lexicon on labeled, in domain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collection = DataCollection.parse_obj(\n",
    "    load_json(get_full_path(\"data/mfc.test.json\"))\n",
    ")\n",
    "in_domain_test_dataset = BagOfWordsSingleBatchDataset(\n",
    "    batch_size=-1,\n",
    "    num_workers=-1,\n",
    "    collection=test_collection,\n",
    "    use_domain_strs=IN_DOMAINS,\n",
    "    vocab_override=lexicon_df[\"word\"].to_list(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.597000002861023"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = lexicon_predict(\n",
    "    lexicon_df=lexicon_df,\n",
    "    dataset=in_domain_test_dataset,\n",
    ")\n",
    "preds = torch.argmax(probs, dim=-1)\n",
    "acc = (\n",
    "    (\n",
    "        preds\n",
    "        == torch.cat(\n",
    "            [batch[\"class_idx\"] for batch in in_domain_test_dataset.get_loader()], dim=0\n",
    "        )\n",
    "    )\n",
    "    * 1.0\n",
    ").mean()\n",
    "acc.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the lexicon on labeled, out of domain data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_domain_test_dataset = BagOfWordsSingleBatchDataset(\n",
    "    batch_size=-1,\n",
    "    num_workers=-1,\n",
    "    collection=test_collection,\n",
    "    use_domain_strs=OUT_DOMAINS,\n",
    "    vocab_override=lexicon_df[\"word\"].to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5249999761581421"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = lexicon_predict(lexicon_df=lexicon_df, dataset=out_domain_test_dataset)\n",
    "preds = torch.argmax(probs, dim=-1)\n",
    "acc = (\n",
    "    (\n",
    "        preds\n",
    "        == torch.cat(\n",
    "            [batch[\"class_idx\"] for batch in out_domain_test_dataset.get_loader()],\n",
    "            dim=0,\n",
    "        )\n",
    "    )\n",
    "    * 1.0\n",
    ").mean()\n",
    "acc.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with the lexicon on partially labeled, out of domain data\n",
    "\n",
    "Use the subset of samples that are labeled to estimate a class distribution of this unseen domain, then use it for domain-specific bias when predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a partially labeled out-of-domain data collection\n",
    "partially_labeled_collection = DataCollection(\n",
    "    class_strs=train_collection.class_strs,\n",
    "    domain_strs=[\"climate\"],\n",
    ")\n",
    "samples = [s for s in train_collection.samples.values() if s.domain_str == \"climate\"]\n",
    "for sample in samples[:250]:  # only first 250 samples are labeled\n",
    "    partially_labeled_collection.add_sample(sample)\n",
    "for sample in samples[250:]:\n",
    "    sample.class_idx = sample.class_str = None\n",
    "    partially_labeled_collection.add_sample(sample)\n",
    "partially_labeled_collection.populate_class_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'climate': [0.06799999999919999,\n",
       "  0.32399999984559996,\n",
       "  0.020000000027999995,\n",
       "  0.008000000035199998,\n",
       "  0.020000000027999995,\n",
       "  0.11999999996799997,\n",
       "  0.004000000037599999,\n",
       "  0.016000000030399995,\n",
       "  0.008000000035199998,\n",
       "  0.036000000018399994,\n",
       "  0.04800000001119999,\n",
       "  0.020000000027999995,\n",
       "  0.17999999993199997,\n",
       "  0.12799999996319997,\n",
       "  3.999999997599999e-11]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution estimated from the subset of samples that are labeled\n",
    "partially_labeled_collection.class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "partially_labeled_dataset = BagOfWordsSingleBatchDataset(\n",
    "    batch_size=-1,\n",
    "    num_workers=-1,\n",
    "    collection=partially_labeled_collection,\n",
    "    vocab_override=lexicon_df[\"word\"].to_list(),\n",
    ")\n",
    "probs = lexicon_predict(\n",
    "    lexicon_df=lexicon_df,\n",
    "    dataset=partially_labeled_dataset,\n",
    ")\n",
    "preds = torch.argmax(probs, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Heat eases, but more waves may come\\nHeat eases, but more waves may come\\nThe heat wave that affected at least 200 million people in the United States during the past week and a half has finally subsided, after shattering or tying thousands of records.\\nNationally, 1,966 daily high maximum temperature records were broken or tied this month (through July 23). More impressive, however, are the figures for nighttime lows. A whopping 4,376 highest minimum temperature records were broken or tied through July 23.\\nIn the Mid-Atlantic, where the the heat peaked July 22-23:\\nl Washington Dulles International Airport broke its all-time record July 22, reaching 105 degrees.\\nl Baltimore Washington International Thurgood Matshall Airport reached its second-highest all-time temperature July 22, 106 degrees.\\nl Washington Reagan National Airport tied its all-time record high minimum temperature of 84 on July 23 and 24.\\nIn many ways, this heat wave exemplified the type of extreme heat events that climate-science studies show are becoming more common in many parts of the world, likely due at least in part to man-made emissions of greenhouse gases. One study, published in the Journal of Geophysical Research in 2008, stated: \"The risk of hot summers is currently rapidly increasing, raising the likelihood of record-breaking heat waves around the world, as seen in Europe in 2003 and 2006 and in North America in 1995 and 2006.\"\\nClimate projections for the rest of this century show that heat waves similar to what we just experienced may become much more common, possibly occurring once or twice per year if emissions continue to increase at present rates.\\n\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample text\n",
    "list(partially_labeled_collection.samples.values())[1000].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 'Capacity and Resources')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted class\n",
    "preds[251].item(), partially_labeled_collection.class_strs[preds[1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "841b8c7c706223fb03b727be38bf91caf98d995881f3f0bfeccf303ad7e8f138"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
