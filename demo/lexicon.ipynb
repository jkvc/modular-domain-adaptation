{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mda.data.bow_dataset import BagOfWordsSingleBatchDataset\n",
    "from mda.data.data_collection import DataCollection\n",
    "from mda.api import lexicon_predict, train_lexicon\n",
    "from mda.util import load_json\n",
    "from repo_root import get_full_path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DOMAINS = [\n",
    "    \"cs.CL\",  # computation and language\n",
    "    \"cs.CV\",\n",
    "    \"cs.LG\",  # machine learning\n",
    "    \"cs.NE\",  # neural\n",
    "    \"cs.SI\",  # social and information network\n",
    "]\n",
    "OUT_DOMAINS = [\n",
    "    \"cs.AI\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collection = DataCollection.parse_obj(\n",
    "    load_json(get_full_path(\"data/arxiv.train.json\"))\n",
    ")\n",
    "train_dataset = BagOfWordsSingleBatchDataset(\n",
    "    batch_size=-1,\n",
    "    num_workers=-1,\n",
    "    collection=train_collection,\n",
    "    use_domain_strs=IN_DOMAINS,\n",
    "    vocab_size=5000,\n",
    ")\n",
    "vocab = train_dataset.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab size\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning',\n",
       " 'data',\n",
       " 'model',\n",
       " 'models',\n",
       " 'network',\n",
       " 'method',\n",
       " 'methods',\n",
       " 'using',\n",
       " 'paper',\n",
       " 'performance']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most common vocab words\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:13<00:00, 67.90it/s]\n"
     ]
    }
   ],
   "source": [
    "lexicon_df = train_lexicon(dataset=train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>upto2008</th>\n",
       "      <th>2009-2014</th>\n",
       "      <th>2015-2018</th>\n",
       "      <th>2019after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>learning</td>\n",
       "      <td>-5.128001e-02</td>\n",
       "      <td>1.680402e-02</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.067732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data</td>\n",
       "      <td>-8.412749e-02</td>\n",
       "      <td>1.026811e-02</td>\n",
       "      <td>0.073600</td>\n",
       "      <td>0.041268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model</td>\n",
       "      <td>-3.845137e-02</td>\n",
       "      <td>-1.000125e-03</td>\n",
       "      <td>0.038513</td>\n",
       "      <td>0.076289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>models</td>\n",
       "      <td>-5.908057e-02</td>\n",
       "      <td>8.870528e-03</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>0.121469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>network</td>\n",
       "      <td>-2.848798e-02</td>\n",
       "      <td>1.328365e-07</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>0.054982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>6d</td>\n",
       "      <td>2.025091e-06</td>\n",
       "      <td>-1.087420e-05</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>0.000078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>equivariance</td>\n",
       "      <td>8.801538e-06</td>\n",
       "      <td>-6.828160e-06</td>\n",
       "      <td>-0.027495</td>\n",
       "      <td>0.088229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>timedependent</td>\n",
       "      <td>-3.742034e-07</td>\n",
       "      <td>-6.719351e-07</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>mot</td>\n",
       "      <td>2.620446e-06</td>\n",
       "      <td>-7.150712e-06</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>sift</td>\n",
       "      <td>-5.447315e-06</td>\n",
       "      <td>2.344010e-02</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>-0.101555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               word      upto2008     2009-2014  2015-2018  2019after\n",
       "0          learning -5.128001e-02  1.680402e-02   0.000004   0.067732\n",
       "1              data -8.412749e-02  1.026811e-02   0.073600   0.041268\n",
       "2             model -3.845137e-02 -1.000125e-03   0.038513   0.076289\n",
       "3            models -5.908057e-02  8.870528e-03   0.008726   0.121469\n",
       "4           network -2.848798e-02  1.328365e-07  -0.000007   0.054982\n",
       "...             ...           ...           ...        ...        ...\n",
       "4995             6d  2.025091e-06 -1.087420e-05  -0.000012   0.000078\n",
       "4996   equivariance  8.801538e-06 -6.828160e-06  -0.027495   0.088229\n",
       "4997  timedependent -3.742034e-07 -6.719351e-07  -0.000011   0.000018\n",
       "4998            mot  2.620446e-06 -7.150712e-06  -0.000032   0.000089\n",
       "4999           sift -5.447315e-06  2.344010e-02  -0.000013  -0.101555\n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('covid19', 1.4918851852416992),\n",
       " ('bert', 1.1331787109375),\n",
       " ('federated', 0.7731612920761108),\n",
       " ('transformer', 0.7572790384292603),\n",
       " ('selfsupervised', 0.7344726920127869),\n",
       " ('transformerbased', 0.6686000823974609),\n",
       " ('pandemic', 0.6529555916786194),\n",
       " ('fewshot', 0.6176268458366394),\n",
       " ('transformers', 0.5596126914024353),\n",
       " ('sota', 0.5254027247428894)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# words with highest weights in papers of 2019 and after\n",
    "sorted(\n",
    "    list(zip(lexicon_df[\"word\"].to_list(), lexicon_df[\"2019after\"].to_list())),\n",
    "    reverse=True,\n",
    "    key=lambda x: x[1],\n",
    ")[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the lexicon on labeled, in domain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collection = DataCollection.parse_obj(\n",
    "    load_json(get_full_path(\"data/arxiv.test.json\"))\n",
    ")\n",
    "in_domain_test_dataset = BagOfWordsSingleBatchDataset(\n",
    "    batch_size=-1,\n",
    "    num_workers=-1,\n",
    "    collection=test_collection,\n",
    "    use_domain_strs=IN_DOMAINS,\n",
    "    vocab_override=lexicon_df[\"word\"].to_list(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 118.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7234599590301514"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = lexicon_predict(\n",
    "    lexicon_df=lexicon_df,\n",
    "    dataset=in_domain_test_dataset,\n",
    ")\n",
    "preds = torch.argmax(probs, dim=-1)\n",
    "acc = (\n",
    "    (\n",
    "        preds\n",
    "        == torch.cat(\n",
    "            [batch[\"class_idx\"] for batch in in_domain_test_dataset.get_loader()], dim=0\n",
    "        )\n",
    "    )\n",
    "    * 1.0\n",
    ").mean()\n",
    "acc.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the lexicon on labeled, out of domain data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_domain_test_dataset = BagOfWordsSingleBatchDataset(\n",
    "    batch_size=-1,\n",
    "    num_workers=-1,\n",
    "    collection=test_collection,\n",
    "    use_domain_strs=OUT_DOMAINS,\n",
    "    vocab_override=lexicon_df[\"word\"].to_list(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 1356.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5878968238830566"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = lexicon_predict(lexicon_df=lexicon_df, dataset=out_domain_test_dataset)\n",
    "preds = torch.argmax(probs, dim=-1)\n",
    "acc = (\n",
    "    (\n",
    "        preds\n",
    "        == torch.cat(\n",
    "            [batch[\"class_idx\"] for batch in out_domain_test_dataset.get_loader()],\n",
    "            dim=0,\n",
    "        )\n",
    "    )\n",
    "    * 1.0\n",
    ").mean()\n",
    "acc.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict with the lexicon on partially labeled, out of domain data\n",
    "\n",
    "Use the subset of samples that are labeled to estimate a class distribution of this unseen domain, then use it for domain-specific bias when predicting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a partially labeled out-of-domain data collection\n",
    "partially_labeled_collection = DataCollection(\n",
    "    class_strs=train_collection.class_strs,\n",
    "    domain_strs=OUT_DOMAINS,\n",
    ")\n",
    "samples = [s for s in train_collection.samples.values() if s.domain_str == OUT_DOMAINS[0]]\n",
    "for sample in samples[:250]:  # only first 250 samples are labeled\n",
    "    partially_labeled_collection.add_sample(sample)\n",
    "for sample in samples[250:]:\n",
    "    sample.class_idx = sample.class_str = None\n",
    "    partially_labeled_collection.add_sample(sample)\n",
    "partially_labeled_collection.populate_class_distribution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cs.AI': [0.036000000034240004,\n",
       "  0.19600000000864,\n",
       "  0.20400000000736,\n",
       "  0.56399999994976]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class distribution estimated from the subset of samples that are labeled\n",
    "partially_labeled_collection.class_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 1842.84it/s]\n"
     ]
    }
   ],
   "source": [
    "partially_labeled_dataset = BagOfWordsSingleBatchDataset(\n",
    "    batch_size=-1,\n",
    "    num_workers=-1,\n",
    "    collection=partially_labeled_collection,\n",
    "    vocab_override=lexicon_df[\"word\"].to_list(),\n",
    ")\n",
    "probs = lexicon_predict(\n",
    "    lexicon_df=lexicon_df,\n",
    "    dataset=partially_labeled_dataset,\n",
    ")\n",
    "preds = torch.argmax(probs, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  The problem of learning Markov equivalence classes of Bayesian network\\nstructures may be solved by searching for the maximum of a scoring metric in a\\nspace of these classes. This paper deals with the definition and analysis of\\none such search space. We use a theoretically motivated neighbourhood, the\\ninclusion boundary, and represent equivalence classes by essential graphs. We\\nshow that this search space is connected and that the score of the neighbours\\ncan be evaluated incrementally. We devise a practical way of building this\\nneighbourhood for an essential graph that is purely graphical and does not\\nexplicitely refer to the underlying independences. We find that its size can be\\nintractable, depending on the complexity of the essential graph of the\\nequivalence class. The emphasis is put on the potential use of this space with\\ngreedy hill -climbing search\\n'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample text\n",
    "list(partially_labeled_collection.samples.values())[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, '2009-2014')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predicted class\n",
    "preds[5].item(), partially_labeled_collection.class_strs[preds[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "841b8c7c706223fb03b727be38bf91caf98d995881f3f0bfeccf303ad7e8f138"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mda')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
